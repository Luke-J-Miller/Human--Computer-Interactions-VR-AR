# Interactive Learning with AR
## Preparation:
Students are encouraged to read the "[Student Preparation](https://github.com/Luke-J-Miller/Human--Computer-Interactions-VR-AR/tree/main/Student%20Preparation)" readme.  This contains valuable information on how to setup accounts that will streamline the learning experience.  Additionally, students are encouraged to bring their own equipment (computers, headsets, cables) so that staff can assist the students getting everything setup for the student to continue and build off of the course.  We have included information on the minimum requirements for computers.
## Weekend 1: Foundations of Unity and ML Integration
[Lesson Slides](https://docs.google.com/presentation/d/1utVYhSMCL8EviY2QNkuyc3JoUDwlz9j4vyebENd_kko/edit?usp=sharing)
### Session 1: Setting Up and Unity Basics
- Objective: Introduce Unity and build a basic interactive scene.

#### Getting Started with Unity
- Download, install, and configure Unity.
- Overview of the Unity interface: Hierarchy, Scene, Game, Inspector, and Project tabs.
#### Building the First Scene 
- Adding objects (3D models, UI elements) and adjusting transforms.
- - Adding physics components: Rigidbodies and colliders.
#### Basic Interactions with Unity
- Adding scripts (in C#) to objects for interactivity.
- Example: Making an object move or react to user input.
#### Hands-On:
- Create a virtual room with basic interactive objects (e.g., a button that triggers an animation).

### Session 2: ML Basics and Unity Integration
- Objective: Introduce ML concepts and incorporate them into Unity.

#### Introduction to ML for Non-Programmers
- Key concepts: What is ML, and how does it apply to AR/VR?
- Pre-trained models and their utility for non-experts.
#### Object Recognition Using ML
Overview of pre-trained object recognition models (e.g., MobileNet).
How ML models process images and make predictions.
#### Integrating ML into Unity 
- Tools: Unity Barracuda or TensorFlow for Unity.
- Example: Setting up a real-time object detection pipeline in Unity.
#### Hands-On:
- Create a Unity scene with a live camera feed and an object detection model that highlights detected objects in AR.

## Weekend 2: Building the Final AR Project
### Session 3: AR Development with Meta Oculuses
- Objective: Set up the Meta Oculus for AR and begin adding AR functionality.

#### Introduction to AR with Unity
- Overview of AR in Unity and how it differs from VR.
- Setting up Unity for AR development (using the Meta Oculus SDK).
#### Augmented Interactions 
- Anchoring virtual objects in real-world spaces.
- Adding basic AR interactions (e.g., grabbing and moving objects).
#### Enhancing AR Scenes 
- Adding animations and sounds to make the experience more engaging.
#### Hands-On:
- Build a scene where virtual objects are anchored to real-world positions and interact with the userâ€™s input.

### Day 4: Finalizing and Presenting the AR Project
- Objective: Combine the elements developed in previous sessions to create a functional AR project.

#### Polishing the Experience 
- Adding UI overlays (e.g., instructions or notifications).
- Debugging and improving interactions for a seamless user experience.
#### Showcase Preparation 
- Participants finalize their projects and test them with peers.
#### Showcase and Feedback 
- Participants present their AR applications to the group.
- Discuss challenges, solutions, and next steps.
#### Hands-On:
- Complete and present the final AR project, incorporating object detection, interactions, and animations.
